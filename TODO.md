# ğŸ›ï¸ Project ODIN: The Linux of AI

> *"Democratizing Intelligence, One Parameter at a Time"*

## ğŸ¯ Mission

Create a decentralized, sovereign, and accessible AI ecosystem that runs on consumer hardware without requiring billion-dollar datacenters.

## ğŸ“‹ Phase 1: Proof of Concept â€” âœ… SCAFFOLD COMPLETE 

**Objective:** Build a 100 Million parameter model based on RWKV-v6 architecture, trained exclusively on synthetic math & coding data, running in the browser via WebAssembly.

> "If this toy can reason, the theory works."

### ğŸ† Architecture Decision: **RWKV-v6** (chosen over Mamba)
- Simpler WASM portability (no selective scan complexity)
- O(N) complexity like Mamba
- Active community & proven at scale
- Better suited for 100M parameter range

---

## ğŸ—ï¸ Project Structure (Implemented)

```
odin/
â”œâ”€â”€ 01_architect_mathematician/    # ğŸ›ï¸ The Visionary
â”‚   â”œâ”€â”€ config.yaml               # Model hyperparameters (768 dim, 12 layers)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ time_mixing.py        # RWKV Time-Mixing with WKV computation
â”‚   â”‚   â”œâ”€â”€ channel_mixing.py     # RWKV Channel-Mixing with squared ReLU
â”‚   â”‚   â””â”€â”€ odin_model.py         # Complete 100M model assembly
â”‚   â”œâ”€â”€ tests/validate_wasm.py    # Numerical validation suite
â”‚   â””â”€â”€ benchmarks/reasoning_benchmark.py
â”‚
â”œâ”€â”€ 02_data_chef/                  # ğŸ§ª The Alchemist  
â”‚   â”œâ”€â”€ generators/
â”‚   â”‚   â”œâ”€â”€ math/arithmetic.py    # +, -, *, / with step-by-step reasoning
â”‚   â”‚   â”œâ”€â”€ math/algebra.py       # Linear/quadratic equations (SymPy)
â”‚   â”‚   â””â”€â”€ code/python_basic.py  # Function implementation problems
â”‚   â”œâ”€â”€ build_dataset.py          # Full dataset builder (1M target)
â”‚   â””â”€â”€ evaluation/generate_testset.py
â”‚
â”œâ”€â”€ 03_distributed_builder/        # ğŸŒ The Swarm Architect
â”‚   â”œâ”€â”€ requirements.txt          # torch, onnx, sympy, tokenizers, wandb
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ train.py              # Training loop with gradient clipping
â”‚   â”‚   â”œâ”€â”€ dataloader.py         # SyntheticMathDataset class
â”‚   â”‚   â””â”€â”€ export_onnx.py        # PyTorchâ†’ONNX export pipeline
â”‚   â””â”€â”€ browser/
â”‚       â”œâ”€â”€ src/runtime.ts        # OdinRuntime with streaming API
â”‚       â””â”€â”€ demo/index.html       # Full demo UI with chat interface
â”‚
â”œâ”€â”€ 04_lowlevel_optimizer/         # âš™ï¸ The Surgeon
â”‚   â”œâ”€â”€ benchmarks/baseline_matmul.py
â”‚   â”œâ”€â”€ wasm_kernels/src/
â”‚   â”‚   â”œâ”€â”€ matmul.rs             # Tiled WASM matmul kernel
â”‚   â”‚   â””â”€â”€ activations.rs        # sigmoid, relu, gelu, softmax
â”‚   â”œâ”€â”€ quantization/int8_quantize.py  # INT8 PTQ pipeline
â”‚   â”œâ”€â”€ memory/browser_memory.py  # <400MB memory optimizer
â”‚   â””â”€â”€ tuning/performance_tuner.py
â”‚
â”œâ”€â”€ ITERATION_PLAN.md             # Task tracking (24/24 complete)
â”œâ”€â”€ pyrightconfig.json            # Pylance configuration
â””â”€â”€ TODO.md                       # This file
```

---

## ğŸ“Š Phase 1 Progress

| Task Block | Architect | Data Chef | Builder | Optimizer | Status |
|------------|-----------|-----------|---------|-----------|--------|
| 1.1 Setup  | âœ… | âœ… | âœ… | âœ… | Complete |
| 2.1 Core   | âœ… | âœ… | âœ… | âœ… | Complete |
| 3.1 Expand | âœ… | âœ… | âœ… | âœ… | Complete |
| 4.1 Integrate | âœ… | âœ… | âœ… | âœ… | Complete |
| 5.1 Browser | âœ… | âœ… | âœ… | âœ… | Complete |
| 6.1 Demo   | âœ… | âœ… | âœ… | âœ… | Complete |

**Total: 24/24 micro-tasks complete** ğŸ‰

---

## ğŸ”§ Model Configuration (100M Parameters)

| Parameter | Value |
|-----------|-------|
| `vocab_size` | 32,768 |
| `embedding_dim` | 768 |
| `num_layers` | 12 |
| `num_heads` | 12 |
| `head_dim` | 64 |
| `ffn_dim` | 2,688 |
| `max_seq_len` | 4,096 |
| **Total Params** | ~100M |

---

## ğŸš€ Next Steps (Execution Phase)

```bash
# 1. Install dependencies
pip install -r 03_distributed_builder/requirements.txt

# 2. Generate synthetic dataset (1M examples)
python 02_data_chef/build_dataset.py

# 3. Train the model
python 03_distributed_builder/src/train.py

# 4. Export to ONNX
python 03_distributed_builder/src/export_onnx.py

# 5. Quantize to INT8
python 04_lowlevel_optimizer/quantization/int8_quantize.py

# 6. Compile WASM kernels
cd 04_lowlevel_optimizer/wasm_kernels
cargo build --target wasm32-unknown-unknown --release

# 7. Launch demo
# Open 03_distributed_builder/browser/demo/index.html
```

---

## ğŸ¯ Success Metrics

| Metric | Target | Status |
|--------|--------|--------|
| Model Size | 100M Â± 5M parameters | âœ… Configured |
| Download Size | < 200MB (INT8) | ğŸ”„ Ready to quantize |
| Inference Speed | > 10 tokens/sec (laptop) | â¬œ Pending training |
| Memory Usage | < 400MB browser | âœ… Optimizer ready |
| Math Accuracy | > 70% on GSM8K-style | â¬œ Pending evaluation |
| Code Accuracy | > 40% on HumanEval-style | â¬œ Pending evaluation |
| Browser Support | Chrome, Firefox, Safari, Edge | âœ… Runtime ready |

---

## ğŸ§¬ Core Philosophy

### Why Not Transformers?
- O(NÂ²) attention is expensive and memory-hungry
- Mamba/RWKV achieve O(N) complexity
- Linear scaling = runs on consumer hardware

### Why Synthetic Data?
- No copyright issues
- No bias from web scraping  
- 100% verified correctness
- Focused on reasoning, not memorization

### Why Browser?
- Zero installation friction
- True decentralization (no server needed)
- Privacy by default (runs locally)
- Proves it works on weak hardware

---

## ğŸ”® Future Phases

- **Phase 2:** Distributed training via P2P swarm
- **Phase 3:** Scale to 1B+ parameters
- **Phase 4:** Multi-modal (vision, audio)
- **Phase 5:** Federated learning network

---

## ğŸ“œ Manifesto

```
We believe:
  - Intelligence should not be controlled by few
  - Privacy is a fundamental right
  - Knowledge should be free and verifiable
  - Small, efficient models beat bloated giants
  - The edge is the future, not the cloud

We reject:
  - Dependency on proprietary hardware
  - Black-box AI systems
  - Surveillance capitalism
  - Artificial scarcity of intelligence
```

---

## ğŸ¤ Contributing

See `ITERATION_PLAN.md` for detailed task breakdown and completion status.

**Phase 1 scaffold is complete.** Ready for:
1. Dataset generation
2. Model training (GPU recommended)
3. ONNX export & quantization
4. Browser deployment

**The revolution will be decentralized.** ğŸ”¥

---

## ğŸ“… Timeline

| Milestone | Status |
|-----------|--------|
| Architecture Design | âœ… Complete (RWKV-v6) |
| Code Scaffold | âœ… Complete (24 files) |
| Data Generators | âœ… Complete |
| Training Pipeline | âœ… Complete |
| WASM Kernels | âœ… Complete |
| Dataset Generation | â¬œ Ready to run |
| Model Training | â¬œ Needs GPU |
| Browser Demo | â¬œ Needs trained model |

---

*Project ODIN - Named after the Norse god who sacrificed an eye for wisdom.
We sacrifice centralization for freedom.*
