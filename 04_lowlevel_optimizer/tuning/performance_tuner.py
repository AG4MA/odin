"""
Performance Tuning Suite
========================
Tools per ottimizzazione finale delle performance ODIN.
"""

import time
import json
from pathlib import Path
from typing import Dict, List, Tuple
from dataclasses import dataclass


@dataclass
class BenchmarkResult:
    """Risultato singolo benchmark"""
    name: str
    tokens_per_second: float
    latency_ms: float
    memory_mb: float
    device: str


class PerformanceTuner:
    """
    Suite di tuning per massimizzare performance su target devices.
    """
    
    def __init__(self):
        self.results: List[BenchmarkResult] = []
        self.optimizations: List[Dict] = []
    
    def benchmark_inference(self, 
                           model,
                           input_length: int = 128,
                           output_length: int = 64,
                           num_runs: int = 10) -> BenchmarkResult:
        """
        Benchmark inference speed.
        
        Measures:
        - Tokens per second (TPS)
        - Time to first token (TTFT)
        - Memory usage
        """
        import torch
        
        device = next(model.parameters()).device
        model.eval()
        
        # Warmup
        dummy_input = torch.randint(0, 32768, (1, input_length), device=device)
        with torch.no_grad():
            for _ in range(3):
                _ = model(dummy_input)
        
        # Benchmark
        if device.type == 'cuda':
            torch.cuda.synchronize()
        
        latencies = []
        for _ in range(num_runs):
            start = time.perf_counter()
            
            with torch.no_grad():
                input_ids = dummy_input
                for _ in range(output_length):
                    logits, _ = model(input_ids[:, -1:])
            
            if device.type == 'cuda':
                torch.cuda.synchronize()
            
            elapsed = time.perf_counter() - start
            latencies.append(elapsed)
        
        avg_latency = sum(latencies) / len(latencies)
        tps = output_length / avg_latency
        
        # Memory
        if device.type == 'cuda':
            memory_mb = torch.cuda.max_memory_allocated() / 1e6
        else:
            memory_mb = 0  # CPU memory tracking requires psutil
        
        result = BenchmarkResult(
            name="inference",
            tokens_per_second=tps,
            latency_ms=avg_latency * 1000,
            memory_mb=memory_mb,
            device=str(device)
        )
        
        self.results.append(result)
        return result
    
    def analyze_bottlenecks(self, model) -> Dict:
        """Identifica bottleneck nelle operazioni"""
        
        import torch
        
        bottlenecks = {
            "time_mixing": {"time_ms": 0, "percentage": 0},
            "channel_mixing": {"time_ms": 0, "percentage": 0},
            "layernorm": {"time_ms": 0, "percentage": 0},
            "embedding": {"time_ms": 0, "percentage": 0},
            "output_head": {"time_ms": 0, "percentage": 0},
        }
        
        # Profile each component (simplified)
        device = next(model.parameters()).device
        dummy = torch.randint(0, 32768, (1, 128), device=device)
        
        # Total forward pass time
        with torch.no_grad():
            start = time.perf_counter()
            _ = model(dummy)
            total_time = (time.perf_counter() - start) * 1000
        
        # Estimate breakdown based on typical RWKV profile
        bottlenecks["time_mixing"]["time_ms"] = total_time * 0.45
        bottlenecks["time_mixing"]["percentage"] = 45
        
        bottlenecks["channel_mixing"]["time_ms"] = total_time * 0.35
        bottlenecks["channel_mixing"]["percentage"] = 35
        
        bottlenecks["layernorm"]["time_ms"] = total_time * 0.10
        bottlenecks["layernorm"]["percentage"] = 10
        
        bottlenecks["embedding"]["time_ms"] = total_time * 0.05
        bottlenecks["embedding"]["percentage"] = 5
        
        bottlenecks["output_head"]["time_ms"] = total_time * 0.05
        bottlenecks["output_head"]["percentage"] = 5
        
        return {
            "total_ms": total_time,
            "breakdown": bottlenecks
        }
    
    def suggest_optimizations(self, bottlenecks: Dict) -> List[Dict]:
        """Suggerisce ottimizzazioni basate sui bottleneck"""
        
        suggestions = []
        
        breakdown = bottlenecks.get("breakdown", {})
        
        # Time mixing is typically the bottleneck
        if breakdown.get("time_mixing", {}).get("percentage", 0) > 40:
            suggestions.append({
                "component": "time_mixing",
                "issue": "WKV computation is the main bottleneck",
                "optimizations": [
                    "Use WASM SIMD for vectorized exp/sigmoid",
                    "Fuse WKV operations to reduce memory bandwidth",
                    "Consider chunked computation for cache locality",
                    "Pre-compute exp(time_decay) outside the loop"
                ],
                "expected_speedup": "1.5-2x"
            })
        
        if breakdown.get("channel_mixing", {}).get("percentage", 0) > 30:
            suggestions.append({
                "component": "channel_mixing",
                "issue": "FFN matmuls are memory-bound",
                "optimizations": [
                    "Use tiled matmul with optimal tile size for L1 cache",
                    "INT8 matmul with dequant fusion",
                    "Reduce FFN dimension if quality permits"
                ],
                "expected_speedup": "1.3-1.5x"
            })
        
        # General optimizations
        suggestions.append({
            "component": "general",
            "issue": "Cross-cutting optimizations",
            "optimizations": [
                "Use Web Workers to keep UI responsive",
                "Stream tokens to reduce perceived latency",
                "Lazy-load model weights by layer",
                "Use SharedArrayBuffer for zero-copy tensor transfer"
            ],
            "expected_speedup": "UX improvement"
        })
        
        self.optimizations = suggestions
        return suggestions
    
    def generate_report(self, output_path: str):
        """Genera report performance completo"""
        
        report = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "benchmarks": [
                {
                    "name": r.name,
                    "tps": r.tokens_per_second,
                    "latency_ms": r.latency_ms,
                    "memory_mb": r.memory_mb,
                    "device": r.device
                }
                for r in self.results
            ],
            "optimizations": self.optimizations,
            "targets": {
                "desktop": {"min_tps": 20, "max_memory_mb": 400},
                "laptop": {"min_tps": 10, "max_memory_mb": 400},
                "mobile": {"min_tps": 3, "max_memory_mb": 300}
            }
        }
        
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"Performance report saved to {output_path}")
        return report
    
    def print_summary(self):
        """Stampa summary performance"""
        
        print("\n" + "=" * 60)
        print("ODIN Performance Tuning Summary")
        print("=" * 60)
        
        for result in self.results:
            print(f"\nðŸ“Š {result.name.upper()}")
            print(f"   Device: {result.device}")
            print(f"   Tokens/sec: {result.tokens_per_second:.1f}")
            print(f"   Latency: {result.latency_ms:.1f} ms")
            if result.memory_mb > 0:
                print(f"   Memory: {result.memory_mb:.0f} MB")
        
        if self.optimizations:
            print("\nâš¡ OPTIMIZATION SUGGESTIONS:")
            for opt in self.optimizations:
                print(f"\n   [{opt['component']}]")
                print(f"   Issue: {opt['issue']}")
                for o in opt['optimizations'][:2]:  # Top 2
                    print(f"   â€¢ {o}")
                print(f"   Expected: {opt['expected_speedup']}")
        
        print("\n" + "=" * 60)


def run_performance_suite():
    """Esegue suite completa di performance tuning"""
    
    print("=== ODIN Performance Tuning Suite ===\n")
    
    tuner = PerformanceTuner()
    
    # Simulated results (in production, run actual benchmarks)
    tuner.results = [
        BenchmarkResult("inference_cpu", 12.5, 5120, 380, "cpu"),
        BenchmarkResult("inference_wasm", 8.2, 7800, 350, "wasm"),
    ]
    
    # Analyze
    bottlenecks = {
        "total_ms": 80,
        "breakdown": {
            "time_mixing": {"time_ms": 36, "percentage": 45},
            "channel_mixing": {"time_ms": 28, "percentage": 35},
            "layernorm": {"time_ms": 8, "percentage": 10},
            "embedding": {"time_ms": 4, "percentage": 5},
            "output_head": {"time_ms": 4, "percentage": 5},
        }
    }
    
    tuner.suggest_optimizations(bottlenecks)
    tuner.print_summary()
    tuner.generate_report("reports/performance_report.json")


if __name__ == "__main__":
    run_performance_suite()
